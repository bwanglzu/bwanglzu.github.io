<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>bo.blog - Fine-tuned CLIP vs Zero-Shot CLIP for search, here are the results</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="bo.blog - Flux ATOM" />
</head>
<body>
<div id="page">

  <header id="header">
    <h1><a href="/index.html">bo.blog</a></h1>
  </header>

<nav id="menu">
  <a href="/index.html">Accueil</a>
  <a href="/archives.html">Archives</a>

</nav> <!-- /#nav -->
  <section id="content">
        
  <article class="post">
        <h2 class="page_title"><a href="fine-tuned-clip-vs-zero-shot-clip-for-search-here-are-the-results.html" rel="bookmark" title="bo.blog - Fine-tuned CLIP vs Zero-Shot CLIP for search, here are the results">Fine-tuned CLIP vs Zero-Shot CLIP for search, here are the results</a></h2>

        <section class="post_content">
        <h2>Background</h2>
<p>The <a href="https://openai.com/blog/clip/">OpenAI CLIP</a> is a neural network trained on a wide variety of images with a wide variety of natural language supervision that’s abundantly available on the internet. It is capable to map text and image embeddings into the same semantic space and make them <em>comparable</em>. i.e. we can measure the similarity between a piece of text and image once we use the CLIP model by encoding them into embeddings.</p>
<p><em>It holds the potential for building a powerful cross-modaity search application</em>.</p>
<p>How? Let's start from the beginning, and ask ourselves: <em>how do we search images with text</em>?</p>
<h2>Text-Image Retrieval: the Traditional Approach</h2>
<p>Search images with text is non-trivial. Text queries normally consist of a list of tokens, while images to search are represented by matrices of values, such as RGB images are represented by three matrices of Red, Green and Blue.</p>
<p>A typical image is represented by three matrices of values.</p>
<p>How can we measure the <em>relatedness</em> between text query and these matrices? There is no way except a lot of manual work.</p>
<p>For instance, social media websites such as <a href="https://www.flickr.com/">Flickr</a> or <a href="https://www.instagram.com/">Instagram</a> allow users to tag their images. Tags can be used for indexing and matching future user queries.
If the tags are missing, some might use pre-trained machine learning models to recognise things within the image, such as dog, cat. Last but not least, if both user tags and classifier are not available, traditionally we have to use the surrounding text of the image to match against text queries, built on the assumption that surrounding text of an image, to some extent, reflected the semantic of the image itself.</p>
<h2>The OpenAI CLIP</h2>
<p>In Jan 2021, Open AI introduced the CLIP (<em>Contrastive Language–Image Pre-training</em>), which efficiently learns how to recognise images of things from natural language supervision. As was introduced in the paper (<a href="https://arxiv.org/pdf/2103.00020.pdf">Learning Transferable Visual Models From Natural Language Supervision</a>),</p>
<blockquote>
<p>CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behaviour to turn CLIP into a zero-shot classifier. We convert all of a dataset’s classes into captions such as “a photo of a <em>dog</em>” and predict the class of the caption CLIP estimates best pairs with a given image.
</p>
</blockquote>
<p>The author created a dataset consist of 400 million image-text pairs for the language-vision pre-training. For the image encoder, a ResNet or ViT was employed as the backbone model, while for text encoder the author used a text transformer. The author claims that:</p>
<blockquote>
<p>Our studies of CLIP in a zero-shot setting show that the model displays significant promise for widely-applicable tasks like image retrieval or search.
</p>
</blockquote>
<p>How CLIP can help Neural Search? The fundenmental difference between Neural Search and Symbolic Search, is, Neural Search find matches on <em>semantics</em>, not <em>occurence</em>. As introduced above, we can use the CLIP text encoder to encode a user query into an embedding, while use the CLIP image encoder to encode all images into embeddings. Then we can apply different similarity/distance metrics to evaluate how similar a query is to all encoded images, and produce a ranking list to return to the user.</p>
<h2>CLIP Fine-tuning with Finetuner</h2>
<p>As introduced in the previous section, CLIP was pre-trained on a large collection of image-text pairs crawled from the internet, this ensures the model itself has a good “zero-shot” capability: it generalise well on a lot of domains.</p>
<p>If you want to apply CLIP on your domain-of-interest, such as Fashion search, Anime search..you might encounter performance issues due to the distribution shift of the training data.</p>
<p>CLIP fine-tuning itself is non-trivial, it involves jointly optimising two models in parallel: the CLIP text encoder and CLIP image encoder on the customized CLIP loss. Let alone a carefully selection of a set of effective hyper-parameters and setting up all the computing devices, such as GPUs.</p>
<p>That’s why we offer <a href="https://github.com/jina-ai/finetuner">Finetuner</a> at Jina AI. Finetuner aims to optimise the quality of embeddings for search tasks. It:</p>
<ul>
<li>Take care all the machine learning algorithms and deep learning techniques, such as contrastive learning, negative sampling, distributed training on top Pytorch Distributed Data Parallel (DDP) etc..</li>
<li>Owns all the complexity to set up computing resources, submit jobs, manage experiments and runs in the cloud.</li>
<li>Deliver an extremely simplified user interface. How easy it is? Take a look at the code block below:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">finetuner</span>

<span class="n">finetuner</span><span class="o">.</span><span class="n">login</span><span class="p">()</span>

<span class="c1"># step 1: fine-tune</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">finetuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;openai/clip-vit-base-patch32&#39;</span><span class="p">,</span>
    <span class="n">train_data</span><span class="o">=</span><span class="s1">&#39;fashion-eval-train-clip&#39;</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">1e-7</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;CLIPLoss&#39;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">run</span><span class="o">.</span><span class="n">stream_logs</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>

<span class="c1"># step 2: inference</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">DocumentArray</span><span class="p">([</span><span class="n">Document</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;white sneaker&#39;</span><span class="p">)])</span>
<span class="n">articles</span> <span class="o">=</span> <span class="n">DocumentArray</span><span class="p">([</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">uri</span><span class="o">=</span><span class="s1">&#39;sneaker1.png&#39;</span><span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">uri</span><span class="o">=</span><span class="s1">&#39;sneaker2.png&#39;</span><span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">uri</span><span class="o">=</span><span class="s1">&#39;sneaker3.png&#39;</span><span class="p">),</span>
<span class="p">])</span>
<span class="c1"># download fine-tuned model</span>
<span class="n">artifact</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">save_artifact</span><span class="p">(</span><span class="s1">&#39;clip-model&#39;</span><span class="p">)</span>
<span class="n">clip_text_encoder</span> <span class="o">=</span> <span class="n">finetuner</span><span class="o">.</span><span class="n">get_model</span><span class="p">(</span><span class="n">artifact</span><span class="o">=</span><span class="n">artifact</span><span class="p">,</span> <span class="n">select_model</span><span class="o">=</span><span class="s1">&#39;clip-text&#39;</span><span class="p">)</span>
<span class="n">clip_image_encoder</span> <span class="o">=</span> <span class="n">finetuner</span><span class="o">.</span><span class="n">get_model</span><span class="p">(</span><span class="n">artifact</span><span class="o">=</span><span class="n">artifact</span><span class="p">,</span> <span class="n">select_model</span><span class="o">=</span><span class="s1">&#39;clip-vision&#39;</span><span class="p">)</span>
<span class="c1"># encode query and articles</span>
<span class="n">finetuner</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">clip_text_encoder</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">query</span><span class="p">)</span>
<span class="n">finetuner</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">clip_image_encoder</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">articles</span><span class="p">)</span>
<span class="c1"># find best match</span>
<span class="n">query</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">articles</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">)</span>
</code></pre></div>

<h2>Does it work? Compare with the Benchmark</h2>
<p>Recently, credits to the <a href="https://laion.ai/">Laion AI</a> team, their engineers published an open source Github repo <a href="https://github.com/LAION-AI/CLIP_benchmark">CLIP Benchmark</a> which evaluated varies CLIP models on three datasets and two tasks: image classification and image retrieval.</p>
<p>The Jina AI team adopted the codebase from CLIP Benchmark and apply Finetuner fine-tuned three variations of CLIP models:</p>
<ol>
<li><code>ViT-B-32#openai</code></li>
<li><code>ViT-B-32-quickgelu#laion400m</code></li>
<li><code>ViT-B-16-plus#laion400m</code></li>
</ol>
<p>on three datasets (same as CLIP Benchmark)</p>
<ol>
<li>Flickr8k</li>
<li>Flickr30k</li>
<li>MS-COCO captions</li>
</ol>
<p>Nine experiments in total, and here are the results (compared with zero-shot CLIP):</p>
<table>
<thead>
<tr>
<th>model</th>
<th>dataset</th>
<th>imageRecall@5(zero-shot)</th>
<th>textRecall@5(zero-shot)</th>
<th>imageRecall@5(fine-tuned)</th>
<th>textRecall@5(fine-tuned)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ViT-B-32#openai</td>
<td>flickr8k</td>
<td>0.532</td>
<td>0.699</td>
<td>0.865</td>
<td>0.908</td>
</tr>
<tr>
<td>ViT-B-16-plus-240</td>
<td>flickr8k</td>
<td>0.644</td>
<td>0.792</td>
<td>0.878</td>
<td>0.920</td>
</tr>
<tr>
<td>ViT-B-32-quickgelu#laion400m_e32</td>
<td>flickr8k</td>
<td>0.579</td>
<td>0.739</td>
<td>0.849</td>
<td>0.902</td>
</tr>
<tr>
<td>ViT-B-32#openai</td>
<td>flickr30k</td>
<td>0.834</td>
<td>0.949</td>
<td>0.902</td>
<td>0.948</td>
</tr>
<tr>
<td>ViT-B-16-plus-240</td>
<td>flickr30k</td>
<td>0.889</td>
<td>0.971</td>
<td>0.917</td>
<td>0.971</td>
</tr>
<tr>
<td>ViT-B-32-quickgelu#laion400m_e32</td>
<td>flickr30k</td>
<td>0.855</td>
<td>0.941</td>
<td>0.872</td>
<td>0.929</td>
</tr>
<tr>
<td>ViT-B-32#openai</td>
<td>coco captions</td>
<td>0.559</td>
<td>0.748</td>
<td>0.655</td>
<td>0.745</td>
</tr>
<tr>
<td>ViT-B-16-plus-240</td>
<td>coco captions</td>
<td>0.662</td>
<td>0.810</td>
<td>0.712</td>
<td>0.814</td>
</tr>
<tr>
<td>ViT-B-32-quickgelu#laion400m_e32</td>
<td>coco captions</td>
<td>0.608</td>
<td>0.768</td>
<td>0.671</td>
<td>0.764</td>
</tr>
</tbody>
</table>
<p>Default hyper-parameters are: <code>learning_rate: 1e-6</code>, <code>epochs: 5</code>, <code>optimizer: Adam</code>.
Flickr models are evaluated on the Karpathy test set.
MS-COCO caption models are fine-tuned on a random subset (100k pairs) extracted from 2014 train images and evaluated on 2014 validation images.</p>
<h2>General insights when Fine-tuning CLIP</h2>
<ul>
<li>Use a small learning rate, such as 1e-6, 1e-7.</li>
<li>You do not need huge or complex models, the ViT-B-32 is good enough with fine-tuning.</li>
<li>If your search case is close domain/different domain, fine-tuning might be a good idea, otherwise not.</li>
<li>Use a Prompt template to turn your keyword into a sentence. For example, use <code>This is a photo of cat</code> instead of <code>cat</code> as your text descriptor.</li>
</ul>
<h2>Credits</h2>
<ul>
<li>Thanks the great work produced by <em><a href="https://github.com/LAION-AI/CLIP_benchmark">CLIP-Benchmark</a>.</em></li>
<li>Thanks <a href="https://github.com/mlfoundations/open_clip">Open-CLIP</a> for providing ~~all~~ pre-trained CLIP models with different weights.</li>
</ul>
<p>If you want to reproduce our results or try Finetuner on your own data, we have created a G<a href="https://colab.research.google.com/drive/1fHSUML3UmrRltzn7xjgl5Db6QOqXjNHa?usp=sharing">oogle Colab</a> together with this blog post. It is worth mentioning that, apart from CLIP, Finetuner is capable of tuning other models like ResNet, EfficientNet and even language models such as BERT.</p>
<p>Please visit our Github page and documentation page for more information:</p>
<ul>
<li><a href="https://github.com/jina-ai/finetuner">https://github.com/jina-ai/finetuner</a></li>
<li><a href="https://finetuner.jina.ai/">https://finetuner.jina.ai/</a></li>
</ul>
        </section>

        <details class="meta">
          Publié le <time datetime="2022-10-03T19:09:00+02:00" pubdate="pubdate">Mon 03 October 2022</time>
 par Bo Wang dans «<a href="/category/thoughts.html">thoughts</a>».  
Mots-clés: <a href="/tag/representation-learning.html">representation learning</a>, <a href="/tag/finetuner.html">finetuner</a></p>        </details>   </article>


  </section> <!-- /#content -->

<aside id="sidebar">

  <div class="widget" id="categories">
    <h2>Catégories</h2>
    <ul>
      <li ><a href="/category/knowledge.html">knowledge</a></li>
      <li class="active"><a href="/category/thoughts.html">thoughts</a></li>
    </ul>
  </div>

  <div class="widget" id="tags">
    <h2>Mots-clés:</h2>
    <ul>
      <li><a href="/tag/representation-learning.html">representation learning</a></li>
      <li><a href="/tag/finetuner.html">finetuner</a></li>
      <li><a class="more" href="/tags.html">Plus...</a></li>
    </ul>
  </div>
  
  <div class="widget" id="blogroll">
    <h2>Liens</h2>
    <ul>
      <li><a href="https://getpelican.com/">Pelican</a></li>
      <li><a href="https://www.python.org/">Python.org</a></li>
      <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
    </ul>
  </div>

    <div class="widget" id="social">
      <h2>Réseaux sociaux</h2>
      <ul>
        <li><a href="https://github.com/bwanglzu">Github</a></li>
        <li><a href="https://twitter.com/bo_wangbo">Twitter</a></li>
       </ul>
    </div>

</aside>

  <footer id="footer">
    <p>Propulsé par <a href="http://docs.notmyidea.org/alexis/pelican/index.html">Pelican</a>.</p>
  </footer>
</div> <!-- /#page -->
</body>
</html>