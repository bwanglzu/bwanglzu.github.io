<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>bo.blog - Nearest Neighbour Search for Document Retrieval - 1</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="bo.blog - Flux ATOM" />
</head>
<body>
<div id="page">

  <header id="header">
    <h1><a href="/index.html">bo.blog</a></h1>
  </header>

<nav id="menu">
  <a href="/index.html">Accueil</a>
  <a href="/archives.html">Archives</a>

</nav> <!-- /#nav -->
  <section id="content">
        
  <article class="post">
        <h2 class="page_title"><a href="nearest-neighbour-search-for-document-retrieval-1.html" rel="bookmark" title="bo.blog - Nearest Neighbour Search for Document Retrieval - 1">Nearest Neighbour Search for Document Retrieval - 1</a></h2>

        <section class="post_content">
        <p>Nowadays, the Neural Search community leverages FAISS/HNSWLIB for approximate nearest neighbor search, and we refer to it as <em>ANN</em>. While building a Neural Search system, <em>ANN</em> search is essential to quickly find a set of articles given millions or articles being indexed in memory or disk. The objective of ANN is to maximize <em>Recall</em>. Typically, we call it <em>Pre-Ranking</em>:</p>
<p><img alt="pipeline" src="images/pipeline"></p>
<p>For those building a Neural Search/Recommender system, the illustration above might look familiar to you.
After preprocessing the articles and encoding all the articles into embeddings, we usually have three stages of Ranking:</p>
<ol>
<li><em>Pre-Ranking</em>: Leverage ANN to find candidate documents from the collections and optimize for Recall (in the dense-retrieval community, refer to it as bi-encoder).</li>
<li><em>Ranking</em>: Given data such as pairs of documents and their similarity score as label produced by pre-ranking step, leverage siamese nets/two-tower nets to optimize rank hinge loss and produce a similarity score (in dense retrieval community refer it as cross-encoder).</li>
<li><em>Post-Ranking</em>: Other ranking measures are given by business requirements or other metrics. Such as, business owners like to put articles with a higher stock rate on top of the rank list or sacrifice the precision/recall to optimize for article diversity.</li>
</ol>
<p>The Nearest Neighbour Search (NN) stays at the core of the pipeline: the quality of the final ranking list is highly dependent on the candidates being generated by the Pre-Ranking step. In this blog, I will cover some basic NN methods for Neural Search.</p>
<p>I'm going to start from KNN, then goes to KD-Trees and finalize with Locality Sensitive Hashing (LSH).</p>
<h2>Motivation</h2>
<p>Imaging we want to build a CBIR (content-based information retrieval) system for a fashion store, which allow user to upload an image of a fashion product to find similar products in stock.</p>
<p><em>How can we achieve that?</em></p>
<p>At indexing time (off-line), we need to index all vision features (could be dense vectors or bag of visual words) as feature vectors. At the searching time (online), when a user posts an image to the search system, we encode the query image into its feature vector and find similar products by computing similarity/distance scores between the query vector and all fashion products.</p>
<p><em>How can we achieve it fast?</em></p>
<p>When your fashion company has limited products, it is doable using brute force search (compute score one-by-one) and produce the final rank list.
What if you have a million products or even more?
We'll answer the question in the following sub-sections.</p>
<p><em>Problem Definition</em></p>
<p>Given a query document <span class="math">\(Q\)</span> and indexed documents <span class="math">\(D_1, D_2, ..., D_N\)</span>, we want to find top-k documents which is sorted based on similarity mearesure:</p>
<div class="math">$$D^{knn} = \max_{D_i}sim(Q, D_i)$$</div>
<p>where <span class="math">\(D^{knn}\)</span> denotes for top-k nearest neightbours, and <span class="math">\(D_i\)</span> denotes for any documents inside the document collection.</p>
<h2>KNN</h2>
<p>K-nearest neighbor is easy. We:</p>
<ol>
<li>first take out <span class="math">\(K\)</span> elements <span class="math">\((D_1, ..., D_k)\)</span> from collection <span class="math">\(D\)</span>.</li>
<li>Compute similarity between <span class="math">\(Q\)</span> and <span class="math">\(K\)</span> elements, and sort them in a DESC order.</li>
<li>Loop through <span class="math">\(D_{k+1}, ..., D_{N}\)</span>, compute the similarity between <span class="math">\(Q\)</span> and each <span class="math">\(D_{i}\)</span>, if the similarity score is greater than the ith elements in K documents, we'll pop the last item in the sorted queue, bubble down the rest items and insert the <span class="math">\(D_i\)</span> into the ith index.</li>
</ol>
<p>But what is the complexity of brute force search? Given an optimized KNN implementation, we can achieve O(NlogK). This could be extremely expensive when the user has an extensive collection of articles to be searched through!
This is especially the case when users expect to get a ranked list as soon as possible!</p>
<h2>KD-Trees</h2>
<p>KD-Trees follow an iterative process to construct the tree.
To make the visualization more straightforward, we suppose the data only consist of two features: <em>f1</em> (x-axis), <em>f2</em> (y-axis), and 50 items. Which looks like this:</p>
<p><img alt="kdtree-0" src="images/kdtree.png"></p>
<h3>Construction of a KD-Tree (Indexing)</h3>
<p><strong>iteration 1</strong></p>
<p>Construction of a KD-Tree starts with selecting a practical feature and setting a threshold for this feature. To illustrate the idea, we begin with a manual selection of <em>f1</em> and a feature threshold of <em>0.5</em>.
To this end, we get a boundary like this:</p>
<p><img alt="kdtree-1" src="images/kdtree-1.png"></p>
<p>As you can see from the figure above, the feature space has been split into two parts by our first selection of <em>f1</em> with a threshold <em>0.5</em>.
How is it reflected for thee <em>tree</em>?</p>
<p>When building the index, we're essentially creating a <em>binary search tree</em>. The first selection of <em>f1</em> with threshold <em>0.5</em> became our root node.
Given each data point, if the <em>f1</em> is greater than .5, it will be placed to the right of the node. Otherwise, we put it to the left of the node.</p>
<p><strong>iteration 2</strong></p>
<p>We continue from the tree above. In the second iteration, let's define our rule as: given <em>f1 &gt; 0.5</em>, select <em>f2</em> with threshold <em>0.5</em>. </p>
<p><img alt="kdtree-2" src="images/kdtree-2.png"></p>
<p>As was shown in the graph above, now we split the feature space again based on the new rule, and it is also reflected on our tree: we created a new Node <em>f2-0.5</em> into the figure (the none Node is only for visualization purpose, we haven't created this Node).</p>
<p><strong>iteration N</strong></p>
<p>If we follow this process, we will end up with:</p>
<p><img alt="kdtree-3" src="images/kdtree-3.png"></p>
<p>As was shown in the above figure, the entire feature space has been split into six bins. Compared with before, we added three new Nodes, including two leaf Nodes:</p>
<ol>
<li>the previous 'none' was replaced by an actual node <code>f2-0.65</code>; this Node split the space of <em>f2</em> based on threshold 0.65, and it only happens when <em>f1&lt;0.5</em>.</li>
<li>when <em>f2&lt;0.65</em>, we further split <em>f1</em> by a threshold of 0.2.</li>
<li>when <em>f2&gt;0.65</em>, we further split <em>f1</em> by a threshold of 0.3.</li>
</ol>
<p>To this end, our tree has three leaf nodes, and each leaf node can construct two bins (less/larger than the threshold), and we have six bins in total.
And each data point can be placed into one of the bins.</p>
<p>Then we finish the construction of the KD-Tree. It should be noted that constructing a KD-Tree could be non-trivial since you need to consider some hyper-parameters, such as how to set the threshold or how many bins we should create (or the stop criteria). In practice, there are no golden rules. Normally mean or median can be used to set the threshold. The number of bins, could be highly dependent on the evaluation of the results and fine-tuning.</p>
<h3>Nearest Neighbour: KD-Tree Searching</h3>
<p>At searching time when a user uploads an image is illustrated as the green x in the image below:</p>
<p><img alt="kdtree-4" src="images/kdtree-search.png"></p>
<p>We can easily place the search article into the <em>top-left bin</em>. Within that bin, we have 1 item that can be considered the candidate's nearest neighbor. At the same time, it's not finished yet.</p>
<p>Given this query data, we also need to compute the minimal distance between the query image and all-other-bins. More specifically, not the bins themselves, but the bounding box (represented as rectangles inside each bins).</p>
<p>If the minimal distance between the query vector is:</p>
<ol>
<li>greater than the distance against the candidate in the same bin, we ignore this bin by pruning the leaf node.</li>
<li>less than the distance against the candidate in the same bin, we add this bin (and all items inside the bin) as candidates.</li>
</ol>
<p>To this end, our search space can be greatly reduced: we do not need to conduct brute-force search, search through several bins is sufficiently enough. In the above case, it is 2.</p>
<h3>Approximate Nearest Neighbour: KD-Tree Searching</h3>
<p>ANN Search for KD-Tree is straightforward. The idea is similar to what we're doing for hard-negative mining: adding a margin to the distance.</p>
<p>In the KD-Tree NN search case, given the query and candidate item in the same bin, let's say the distance between them is <span class="math">\(d(q, candidate)\)</span>.
After then, we also compute the minimal distance between the query and the bounding boxes. In the above example, we'll get five other distances.
If the distance between the query and the bounding box is less than <span class="math">\(d(q, candidate)\)</span>, we include all the items inside the bin.</p>
<p>ANN search works exactly the same, while we penalize the <span class="math">\(d(q, candidate)\)</span> a little bit by divide <span class="math">\(d(q, candidate)/factor\)</span> where <span class="math">\(factor \in [1, \infty)\)</span>.</p>
<p>In this case, if <em>factor=1</em>, it is identical to the NN search. If <em>factor&gt;1</em>, the distance between query and bounding boxes has to be even smaller than the penalized value to be considered as <em>nearest neighbors</em>.</p>
<p>In practice, KD-Trees suffer from the curse of dimensionality.
It is tricky to apply them to high-dimensional data.
Some variations have been adapted to solve the issue. For example, <a href="https://github.com/spotify/annoy">Annoy</a> implements a Randomized KD-Tree forest:</p>
<blockquote>
<p>Using random projections and by building up a tree. At every intermediate node in the tree, a random hyperplane is chosen, which divides the space into two subspaces. This hyperplane is chosen by sampling two points from the subset and taking the hyperplane equidistant from them.
We do this k times so that we get a forest of trees. k has to be tuned to your need, by looking at what tradeoff you have between precision and performance.</p>
</blockquote>
<h2>Locality Sensitive Hashing</h2>
<p>Locality Sensitive Hashing can be an excellent alternative for NN search.
It is still one of the most widely used ANN search algorithms, given a million-scale data point to be indexed. Similar to before, we'll start from index construction.</p>
<p>We will use the same dataset as used above: 2 features <em>f1</em> and <em>f2</em>, with 50 data points in the 2-dimensional space:</p>
<p><img alt="kdtree-0" src="images/kdtree.png"></p>
<h3>Construction of LSH Bins (Indexing)</h3>
<p>At indexing time, we first need to create random planes (hyperplanes) to split the feature space into <em>bins</em>.</p>
<p><img alt="lsh-0" src="images/lsh-0.png"></p>
<p>In the figure above, we have created six hyperplanes. Each hyper-plane can split our feature space into two bins: either left/right or up/bottom, which can be represented as binary codes (or signs): <em>0</em> or <em>1</em>. This binary code forms the index of a bin.</p>
<p>Let's try to get the bin index of the bottom right bin (which has 4 points in the bin); it is located on the:</p>
<ol>
<li>right of the <em>plane1</em>, the sign at position 0 is 1.</li>
<li>right of the <em>plane2</em>, sign at position 1 is 1.</li>
<li>right of the <em>plane3</em>, sign at position 2 is 1.</li>
<li>right of the <em>plane4</em>, sign at position 3 is 1.</li>
<li>bottom of the <em>plane5</em>, sign at position 4 is 0.</li>
<li>bottom of the <em>plane6</em>, sign at position 5 is 0.</li>
</ol>
<p>So that we can represent the bottom-right bin as <code>111100</code>, if we iterative this process and annotate each bins with a bin index, we'll end up with a hash map: The keys of the hash map is the bin index, while the values of the hash map are the ids of the data points within the bin.</p>
<p>Now we finished the process of index construction of LSH.</p>
<p><img alt="lsh-0" src="images/lsh-1.png"></p>
<h3>Nearest Neighbour: Searching through Bins</h3>
<p>Searching using LSH is easy. Intuitively, given a query, you can search all data points within its own bin or search through its neighboring bins.</p>
<p>How to search through its neighboring bins? Take a look at the figure above. The bin index is represented as binary codes. The neighboring bins will only have 1 bit of mismatch against its own bin index.</p>
<p>Apparently, you can consider this as a hyper-parameter to search through more neighboring bins.</p>
<p>I'll conclude the blog here. The next time, we'll move into a billion-scale ANN search beyond KD-Trees and Locality Sensitive hashing, i.e., Graph Traversal based algorithms and Product Quantization.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>

        <details class="meta">
          Publié le <time datetime="2022-08-14T16:48:00+02:00" pubdate="pubdate">Sun 14 August 2022</time>
 par Bo Wang dans «<a href="/category/knowledge.html">knowledge</a>».  
Mots-clés: <a href="/tag/nearest-neighbour-search.html">nearest neighbour search</a></p>        </details>   </article>


  </section> <!-- /#content -->

<aside id="sidebar">

  <div class="widget" id="categories">
    <h2>Catégories</h2>
    <ul>
      <li class="active"><a href="/category/knowledge.html">knowledge</a></li>
      <li ><a href="/category/thoughts.html">thoughts</a></li>
    </ul>
  </div>

  <div class="widget" id="tags">
    <h2>Mots-clés:</h2>
    <ul>
      <li><a href="/tag/nearest-neighbour-search.html">nearest neighbour search</a></li>
      <li><a class="more" href="/tags.html">Plus...</a></li>
    </ul>
  </div>
  
  <div class="widget" id="blogroll">
    <h2>Liens</h2>
    <ul>
      <li><a href="https://getpelican.com/">Pelican</a></li>
      <li><a href="https://www.python.org/">Python.org</a></li>
      <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
    </ul>
  </div>

    <div class="widget" id="social">
      <h2>Réseaux sociaux</h2>
      <ul>
        <li><a href="https://github.com/bwanglzu">Github</a></li>
        <li><a href="https://twitter.com/bo_wangbo">Twitter</a></li>
       </ul>
    </div>

</aside>

  <footer id="footer">
    <p>Propulsé par <a href="http://docs.notmyidea.org/alexis/pelican/index.html">Pelican</a>.</p>
  </footer>
</div> <!-- /#page -->
</body>
</html>