[{"categories":null,"content":"Background The OpenAI CLIP is a neural network trained on a wide variety of images with a wide variety of natural language supervision that’s abundantly available on the internet. It is capable to map text and image embeddings into the same semantic space and make them comparable. i.e. we can measure the similarity between a piece of text and image once we use the CLIP model by encoding them into embeddings.\nIt holds the potential for building a powerful cross-modaity search application.\nHow? Let’s start from the beginning, and ask ourselves: how do we search images with text?\nText-Image Retrieval: the Traditional Approach Search images with text is non-trivial. Text queries normally consist of a list of tokens, while images to search are represented by matrices of values, such as RGB images are represented by three matrices of Red, Green and Blue.\nA typical image is represented by three matrices of values.\nHow can we measure the relatedness between text query and these matrices? There is no way except a lot of manual work.\nFor instance, social media websites such as Flickr or Instagram allow users to tag their images. Tags can be used for indexing and matching future user queries. If the tags are missing, some might use pre-trained machine learning models to recognise things within the image, such as dog, cat. Last but not least, if both user tags and classifier are not available, traditionally we have to use the surrounding text of the image to match against text queries, built on the assumption that surrounding text of an image, to some extent, reflected the semantic of the image itself.\nThe OpenAI CLIP In Jan 2021, Open AI introduced the CLIP (Contrastive Language–Image Pre-training), which efficiently learns how to recognise images of things from natural language supervision. As was introduced in the paper (Learning Transferable Visual Models From Natural Language Supervision),\nCLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behaviour to turn CLIP into a zero-shot classifier. We convert all of a dataset’s classes into captions such as “a photo of a dog” and predict the class of the caption CLIP estimates best pairs with a given image.\nThe author created a dataset consist of 400 million image-text pairs for the language-vision pre-training. For the image encoder, a ResNet or ViT was employed as the backbone model, while for text encoder the author used a text transformer. The author claims that:\nOur studies of CLIP in a zero-shot setting show that the model displays significant promise for widely-applicable tasks like image retrieval or search.\nHow CLIP can help Neural Search? The fundenmental difference between Neural Search and Symbolic Search, is, Neural Search find matches on semantics, not occurence. As introduced above, we can use the CLIP text encoder to encode a user query into an embedding, while use the CLIP image encoder to encode all images into embeddings. Then we can apply different similarity/distance metrics to evaluate how similar a query is to all encoded images, and produce a ranking list to return to the user.\nCLIP Fine-tuning with Finetuner As introduced in the previous section, CLIP was pre-trained on a large collection of image-text pairs crawled from the internet, this ensures the model itself has a good “zero-shot” capability: it generalise well on a lot of domains.\nIf you want to apply CLIP on your domain-of-interest, such as Fashion search, Anime search..you might encounter performance issues due to the distribution shift of the training data.\nCLIP fine-tuning itself is non-trivial, it involves jointly optimising two models in parallel: the CLIP text encoder and CLIP image encoder on the customized CLIP loss. Let alone a carefully selection of a set of effective hyper-parameters and setting up all the computing devices, such as GPUs.\nThat’s why we offer Finetuner at Jina AI. Finetuner aims to optimise the quality of embeddings for search tasks. It:\nTake care all the machine learning algorithms and deep learning techniques, such as contrastive learning, negative sampling, distributed training on top Pytorch Distributed Data Parallel (DDP) etc.. Owns all the complexity to set up computing resources, submit jobs, manage experiments and runs in the cloud. Deliver an extremely simplified user interface. How easy it is? Take a look at the code block below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import finetuner finetuner.login() # step 1: fine-tune run = finetuner.fit( model='openai/clip-vit-base-patch32', train_data='fashion-eval-train-clip', epochs=5, learning_rate= 1e-7, loss='CLIPLoss', device='cuda', ) for entry in run.stream_logs(): print(entry) # step 2: inference query = DocumentArray([Document(text='white sneaker')]) articles = DocumentArray([ Document(uri='sneaker1.png'), Document(uri='sneaker2.png'), Document(uri='sneaker3.png'), ]) # download fine-tuned model artifact = run.save_artifact('clip-model') clip_text_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text') clip_image_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision') # encode query and articles finetuner.encode(model=clip_text_encoder, data=query) finetuner.encode(model=clip_image_encoder, data=articles) # find best match query.match(articles, limit=1, metric='cosine') Does it work? Compare with the Benchmark Recently, credits to the Laion AI team, their engineers published an open source Github repo CLIP Benchmark which evaluated varies CLIP models on three datasets and two tasks: image classification and image retrieval.\nThe Jina AI team adopted the codebase from CLIP Benchmark and apply Finetuner fine-tuned three variations of CLIP models:\nViT-B-32#openai ViT-B-32-quickgelu#laion400m ViT-B-16-plus#laion400m on three datasets (same as CLIP Benchmark)\nFlickr8k Flickr30k MS-COCO captions Nine experiments in total, and here are the results (compared with zero-shot CLIP):\nmodel dataset imageRecall@5(zero-shot) textRecall@5(zero-shot) imageRecall@5(fine-tuned) textRecall@5(fine-tuned) ViT-B-32#openai flickr8k 0.532 0.699 0.865 0.908 ViT-B-16-plus-240 flickr8k 0.644 0.792 0.878 0.920 ViT-B-32-quickgelu#laion400m_e32 flickr8k 0.579 0.739 0.849 0.902 ViT-B-32#openai flickr30k 0.834 0.949 0.902 0.948 ViT-B-16-plus-240 flickr30k 0.889 0.971 0.917 0.971 ViT-B-32-quickgelu#laion400m_e32 flickr30k 0.855 0.941 0.872 0.929 ViT-B-32#openai coco captions 0.559 0.748 0.655 0.745 ViT-B-16-plus-240 coco captions 0.662 0.810 0.712 0.814 ViT-B-32-quickgelu#laion400m_e32 coco captions 0.608 0.768 0.671 0.764 Default hyper-parameters are: learning_rate: 1e-6, epochs: 5, optimizer: Adam. Flickr models are evaluated on the Karpathy test set. MS-COCO caption models are fine-tuned on a random subset (100k pairs) extracted from 2014 train images and evaluated on 2014 validation images.\nGeneral insights when Fine-tuning CLIP Use a small learning rate, such as 1e-6, 1e-7. You do not need huge or complex models, the ViT-B-32 is good enough with fine-tuning. If your search case is close domain/different domain, fine-tuning might be a good idea, otherwise not. Use a Prompt template to turn your keyword into a sentence. For example, use This is a photo of cat instead of cat as your text descriptor. Credits Thanks the great work produced by CLIP-Benchmark. Thanks Open-CLIP for providing all pre-trained CLIP models with different weights. If you want to reproduce our results or try Finetuner on your own data, we have created a Google Colab together with this blog post. It is worth mentioning that, apart from CLIP, Finetuner is capable of tuning other models like ResNet, EfficientNet and even language models such as BERT.\nPlease visit our Github page and documentation page for more information:\nhttps://github.com/jina-ai/finetuner https://finetuner.jina.ai/ ","description":"","tags":null,"title":"Fine-tuned CLIP vs Zero-Shot CLIP for search, here are the results","uri":"/posts/clip-finetune/"},{"categories":null,"content":"Background Deep learning for Information Retrieval is non-trivial. Most of the people who have a background in Information Retrieval in industry lack of proper deep learning knowledge. On the other hand, people research deep learning care a lot about classification, segmentation rather than search.\nOn the other hand, in the real-world, there is a urgent need for deep learning powered search. This is especially the case for Content-based Image Retrieval (CBIR, i.e. find similar images) or Multi/Cross-modal Information Retrieval, even Cross-lingual Information Retrieval.\nIn practice, traditional search algorithms such as Okapi-Bm25 already did a good job in text-to-text search, incorporating dense vectors for search result re-ranking (2nd-stage ranking) has been proved to bring much better search quality. This blog explains how Google brings Bert model into Google search.\nFor CBIR, the “old” ways is to enable bag of visual words and in the recent years, we use deep neural networks to extract visual representation and find similar images using similarity/distance metrics.\nMost of the web shops consider each item as a Product rather than a single image or a piece of text descriptor. In this case, combining image features and text features to leverage multi-modality representation becomes an essential topic.\nBesides these common use cases, cross-lingual retrieval over different marketplace, short-video retrieval, music retrieval forms several niche market.\nEach of these search applications, is powered by a research community. This research community train big models on a general task. But bring this model to your production environment could be painful: You’ll soon realize leveraging pre-trained model produce trash search results.\nWhy? In research people name it distribution shift. The model trained on the dataset, let’s say ImageNet, contains millions of images of real-world objects, rather than the data within your niche market. So we need to fine-tune this model a bit to adopt the data distribution of your dataset.\nYou might ask, okay, i know there are a bunch of frameworks/services already exist, and they can help me fine-tune a model, why you build Finetuner and turn it into a service?\nWhat’s the difference between Finetuner and other Libraries? Once there was a Jina AI community member ask a question:\nWhat’s the difference between Fientuner and Huggingface Transformers Trainer and other frameworks?\nI believe the major difference is we a producing an embedding model. This is because for search tasks, the most important thing is the quality of embedding/vector/representation.\nFor most of the frameworks, such as Huggingface transformers, fine-tuning means applies transfer learning with a new head. This head is, your task of interest. More specifically, if you take a pre-trained VisionTransformer on ImageNet, and want to fine-tune it for, let’s say, cat-dog classification. You head will becomes a 2-class classification head, rather than 1000 class ImageNet head.\nFinetuner, however, respect your choice of model. It could be a vision model from timm, torchvision, or transformers. When receiving your model of choice, Finetuner will try to interpret your model architecture and “cut” your model head or applying pooling, then convert your model into an embedding model. This model, receiving an input, produce a feature vector to represent your data. Finetuner try to learn the data distribution of your data and improve the quality of the learned embedding/vector/representation.\nIf you want to know more about how Fituner works in general, please refer to my previous blog.\nWhy Finetuner becomes a service and How did we make the change? Several important reasons, the most important one is, Finetuner-Service will become a stakeholder to power several Jina applications internally, such as DocsQA, Now, Clip-As-Service and some unannounced project.\nThese applications do not have to care about the usage, Finetuner will hide the complexity of machine learning, and these applications deliver better search results to the user.\nHowever, we would also release the service externally to help Jina users improve their search quality. Our design objective is:\nWe care about model performance, but we’re not chasing SOTA. We try to find a balance point between performance and stability. User stay inside Jina ecosystem, docarray is the entrypoint for preparing training data. Minimum configuration required, while allow user to control their experiments. Previously, the Finetuner team within Jina is maintaining a single repo Finetuner. This OSS version will be abandoned, given a small team, in the past 3.5 months, this is what we have now:\nA much improved (close sourced) Finetuner-Core, with transformer support, CLIP support, much faster training on a single GPU or across multi-gpus, bug fixes and refactoring. A brand-new (close sourced) Finetuner-API, handles all cloud jobs, resource management, Finetune-job submission and deep integration with Jina’s user system. A brand-new (open sourced) Finetuner-Client, allow user to create configurations and talk with Finetuner-API. In short, the heavy job has been delegrated to Finetuner-API and Finetuner-Core, and user only need to talk with Finetuner-Client.\nHow easy it is? Please check the screenshot below:\nNote, it’s not yet published to Pypi.\nNot bad, right? Actually, user only need to provide a model name and train_data.\nOnce you logged in, Finetuner will push your data to your own artifact storage (only visible to you), reserve cloud resources such as CPU/GPU/Memory, start the job and push fine-tuned model, again, to your artifact storage. Once training is over, you can call save_model to pull your model back to your machine. That’s it!\nHow much performance Gain You Can Expect? It depends on a lot of factors. Internally we have conducted a lot of experiments on varies tasks, such as image-to-image search, text-to-text search, cross-modal search. Across these three tasks, Finetuner is able to boost 20%-45% of precision@k and recall@k. You can also observe significant performance improvement on other metrics, such as mean recipal rank (mRR) or normalized discounted cumulative gain (nDCG).\nThat’s it for today, thanks for your time, we’ll soon release the latest finetuner, stay tuned.\n","description":"","tags":null,"title":"Our Journey Towards Build Model Finetuning as Service","uri":"/posts/finetune-as-service/"},{"categories":null,"content":"Instance-level image retrieval is an important component of image retrieval tasks. Given an image as Query, an instance retrieval system aims at find the same objects D with respect to Q. For instance, given an image of the Great Wall, instance retrieval should be able to find other Great Wall images, under different circumstances. Or return facial images of the same person while querying with another facial image of a celebrity.\nFor a long period of time, (Supervised)Metric-Learning has always been the answer:\nPrepare a training set, with class labels. Construct triplets (or tuples, or more..), each triplet contains an anchor image, a positive and a negative. Apply triplet margin loss by maximizing the distance metric between anchor and negative pair, while pull anchor and positive closer. source: FaceNet: A Unified Embedding for Face Recognition and Clustering.\nApparently, supervised deep metric learning is more than that. In the past months, while developing Finetuner at Jina AI, we went through a lot of traps.\nFor instance, Should we allow user to manually create “hard-negatives”? The answer is a clear No. But in the first iteration, the input of our data is Document with 2 Matches like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from docarray import Document anchor = Document( uri='this-is-the-anchor-image.jpg', ) pos = Document( uri='this-is-an-positive-image', tags={'label': 1} # positive share the same label as anchor ) neg = Document( uri='this-is-a-negative-image', tags={'label': -1} # negative has an different label ) anchor.matches.extend([pos, neg]) This “gives” user the power to perform manual positive/negative selection. However, does it make any sense? Probably not for several reasons:\nIt takes efforts for users to perform negative mining, in another word, it take times. Since it might takes time, user might just randomly sampling negatives from other classes. Negative samples might not being effectively used. Suppose user select 1 positive 4 negatives, we end up with 4 triplets, not much, quantity wise. Negative samples might not being selected has hard-negatives, given in most cases, hard-negatives brings more benefits while model training. What is the better approach? Do it automatically!\nSuppose you have a deep metric learning training task, your batch size is 128, this is what we offer you:\nFor each batch of 128 samples, we sample N items from the same class, e.g. 4. In the end, you got 32 classes within batch. Consider each item as an anchor image, then you get 3 positives (4 - 1). Consider each item as an anchor image, then you get 124 negatives (128 - 3 - 1). In the end, within this batch, how many triplets you get? Much larger than 4 if we perform manual negative selection (1 anchor-1 pos-4 neg)! Now you have a set of triplets, you can either:\nTraining your model with all automatically selected triplets (with no mining). Training your model with different mining strategies (easy/semi-hard/hard negative mining). This makes training a metric learning model much more effectively. While you might still discover some minor issues:\nIf we want to apply hard-negative mining, and if I can only perform hard-negative mining within the batch, this means I can only select the “best” hard-negative within a batch, not over the entire dataset.\nThis is true, and 2 possible solutions: either you increase your batch size to generate a bigger “pool size” and reduce the issue, or you perform cross-batch hard-negative mining. Previous solution is easier, while could be limited by your hard-ware capacity. After all, GPU-memory is too limited compare with RAM. The later strategy, could resolve the issue. While it is tricky to bring the code into a reusable component of a software.\nWe’ve approved that deep metric learning + triplet margin loss + sampling + hard-negative mining is extremely useful, and, it is especially the case, for instance-level image retrieval.\nSince 2018, Google launched Google Landmark Retrieval 2021. This is a perfect playground for instance-level image retrieval.\nIf you look at the top solutions, almost all of them use deep metric learning and some add-on tricks. Includes different loss functions, pooling layers etc. Nothing other than deep metric learning. The latest winner, according to leaderboard, reached a mAP@100 of 0.53751.\nIf you employ a pre-trained ResNet/EfficientNet trained on ImageNet dataset, remove the last classification layer and turn it into a Embedding model (feature extractor), you can get roughly 0.25 - 0.30 mAP@100. Quite significant improvement, right?\nTo now, we’ve discussed the basic idea of deep metric learning and how good it performs on Instance Retrieval task. You might ask, what about self-supervised learning (SSL), are you still going to mention that? I say yes, this is the turning point.\nSSL has been a part of representation learning for some time. But we discovered significant progress has been made in the year of 2021. Such as SimCLR, SImCLRv2, MoCo, MocoV2, BYOL etc. These approaches aimes at learn a good representation (embedding model) with NO labels. AND THIS IS HUGE IN REAL-WORLD CASES.\nAs a machine learning engineer, How many times have you tried to convince your BOSS the importance of the labeling? How many times have you tried to label by yourself? Have you tried convince your manager to spend some money on crowdsourcing platforms to get labels for you? Are you tired of, build a user-in-the-loop pipeline to deploy a not-perfect model first, and expect to collect more labels as time pass by?\nIf non of above make any sense to you, good for you. But, what inspiring me most, is finally, we have a SSL paper evaluated on Instance Retrieval Task! This is published by facebook: Emerging Properties in Self-Supervised Vision Transformers. We’ll call it DINO in the following section. Let’s first see some evaluation result:\nIf you look at the results without reading the paper, it might confuse you. Basically, they trained the model on Google Landmark Dataset we mentioned above. While evaluated results on Oxford and Paris dataset (another 2 smaller version of landmark datasets). And they claim the results are even better than some supervised approaches.\nHow did they achieve it without using any labels?\nTrick1: Multi-stage cropping. The basic idea of self-supervised learning is, given an image, it apply augmentation on this image two times, and generate 2 different Views of the same image. Even through these two different Views of the same image looks different, but we know they’re from the same classes: we augmented the image. Multi-stage cropping is another step: we crop the input image, let’s say 6 times. 2 times we use Global Cropping and we perform Local Cropping 4 times. Global cropping basically crops over 50% percent of the original image, while local cropping crops around 20-30% of the original image. After cropping, we transform the GlobalCropping back to 224 by 224, LocalCropping to 96 by 96.\nTrick2: Patches and Vision Transformer. Once augmentation finished, we split the images into patches (a small portion of the augmented image). Feed patches as inputs to a Vision Transformer model. The final [CLS] token of 768d becomes the learned representation.\nTrick3: Self-distillation and momentum update. Distillation normally involves 2 models, one smaller-sized student model and one larger sized teacher model. For DINO, the author used two identical models (ViT) for both student and teacher. While apply stop-gradient for teacher model (no back backpropagation), while update teacher model in an exponential moving average manner. EMA places a greater weight and significance on the most recent data points. At early stage, student network post a bit more significant impact on teacher networks’ parameters. As training goes on, the student network has less-and-less impact on teacher network, until training finish.\nTrick4: Centering and Sharpening. According to previous SSL research, when training model with no negative samples (only tuples) can result in model collapse. The model can not distinguish which is good, which is bad since we only have positive samples. The author introduced Centering parameter C to keep the running average of the rated teacher network ouputs. And applied always deduct this running average when computing normalized cross-entropy loss.\nHow does is final representation looks like?\nTraining models without labels, for representation learning, evaluated on search task, and get very impressive results. These are the reasons I consider SSL could becomes a game changer.\nHow? It might change the way we deploy ML solutions for varies downstream tasks, such as search task. Currently, when user has limited amount of training data, the best situation seems to be: Use pre-trained model, freeze most of the layers, apply transfer learning + metric learning for model finetunning.\nSuppose user provided large amount of messy training data without any labels, what can you do? The only possible way seems to be: combine Active Learning and Transfer Learning. User label the data interactively to provide labeled training samples. This could be useful, but lack of theoretical evaluations and practical support.\nOne would argue, in most of my use cases, I always have labeled data. This could be a valid point in today’s search industries. But to now, when applying deep learning on image retrieval, the market is super biased towards to fashion industry, and this industry, has a lot of labeled data already.\nWhile I believe, with time pass by, we’ll soon see the shift of vector search from fashion, to more fields. Such as retail, tourism, real-estate, 3d meshes etc. Is our data/model ready for these industries already? The answer is clearly a no.\nThink from another point, what if, the self-supervised learning approaches, without any supervision, could bring on-par performance compared with supervised approaches? It’s gonna an interesting case, then the only point for using metric-learning is because it’s faster and consume less time/power.\nStop from here. In the next blog, I’ll write another one brings more code and quantitative evaluation. We’ll get some better understanding of SSL vs Metric Learning by conducting some inspection on their failure cases.\n","description":"","tags":null,"title":"Metric-Learning vs Self-Supervised Learning, which produce better embeddings for Instance Retrieval?","uri":"/posts/metrics-vs-ssl/"}]
