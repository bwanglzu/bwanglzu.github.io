Title: Nearest Neighbour Search for Document Retrieval - 1
Date: 2022-08-14 16:48
Tags: nearest neighbour search
Category: knowledge
Authors: Bo Wang
Summary: Nearest Neighbour Search for Document Retrieval: the fundamentals

Nowadays, the Neural Search community leverages FAISS/HNSWLIB for approximate nearest neighbor search, we refer it as *ANN*.
While building a Neural Search system, *ANN* search is essential to quickly find a set of articles given millions or articles being indexed in memory or disk. The objective for *ANN* is to maximize *Recall*. Normally, we call it *Pre-Ranking*:

![pipeline](images/pipeline)

For those who is building a Neural Search/Recommender system, the above illustratation might looks familar to you.
After preprocessing the articles and encode all the articles into embeddings, we normally have 3 stages of Ranking:

1. *Pre-Ranking*: Leverage ANN to find candidate documents from the collections, optimize for recall (in dense-retrieval community refer it as bi-encoder).
2. *Ranking*: Given data such as pairs of documents and their similary score as label produced by pre-ranking step, leverage siamese nets/two-tower nets to optimize rank hinge loss and produce a similary score (in dense retrieval community refer it as cross-encoder).
3. *Post-Ranking*: Other ranking mearesures given by business requirements or other metrics. Such as business owners like to put articles with higher stock rate on top of the rank list, or sacrifice the precision/recall to optimize for artical diversity.

The Nearest Neighbour Search (NN) stays at the core of the pipeline: the quality of the final ranking list hightly dependent on the candidates being generated by the Pre-Ranking step. In this blog, I'm going to cover the some basics of Neural Search, the era of Nearest Neighbour Search without *A*.

I'm going to start from KNN, then goes to KD-Trees and fianlzing with Locally Sensitive Hashing (LSH) in the next blog.

## Motivation

Imaging we want to build a CBIR (content-based information retrieval) system for a fashion store, which allow user to upload an image of a fashion product to find similar products in stock.

*How can we achieve that?*

At indexing time (off-line), we need to index all vision features (could be dense vectors or bag of visual words) as feature vectors. At searching time (on-line), when user post a image to the search system, we encode the query image into it's feature vector and find similar products by compute similary/distance scores between query vector and all fashion products.

*How can we achieve it fast?*

When your fashion company has limited products, it is doable using brute force search (compute score one-by-one) and produce the final rank list.
What if you have a million products or even more?
We'll answer the question in the following sub-sections.

*Problem Definition*

Given a query document $Q$ and indexed documents $D_1, D_2, ..., D_N$, we want to find top-k documents which is sorted based on similarity mearesure:

$$D^{knn} = \max_{D_i}sim(Q, D_i)$$

where $D^{knn}$ denotes for top-k nearest neightbours, and $D_i$ denotes for any documents inside the document collection.

## KNN

K-nearest neighbour is easy, we:

1. first take out $K$ elements $(D_1, ..., D_k)$ from collection $D$.
2. Compute similarity between $Q$ and $K$ elements, sort them in a DESC order.
3. Loop through $D_{k+1}, ..., D_{N}$, compute similarity between $Q$ and each $D_{i}$, if the similarity score is greater than the ith elements in K documents, we'll pop the last item in the sorted queue, bubble down the rest items and insert the $D_i$ into the ith index.

But what is the complexity of brute force search? Given an optimized KNN impelemntation, we can achieve O(NlogK). This could be extremely expensive when the user has a large collection of articles to be searched through!
This is especially the case when users are expecting to get a ranked list as soon as possible!

## KD-Trees

KD-Trees follows an iterative process to construct the tree.
To make the visualization easier, we suppose the data only consist 2 features: *f1* (x-axis), *f2* (y-axis) and 50 items. Which looks like this:

![kdtree-0](images/kdtree.png)

### Construction of a KD-Tree (Indexing)

**iteration 1**

Construction of a KD-Tree first start from selecting a proper feature and set a threshold for this feature. To illustrate the idea, we start from manual selection of *f1* and a feature threshold of *0.5*.
To this end, we get a boundary like this:

![kdtree-1](images/kdtree-1.png)

As you can see from the figure above, the feature space has been splitted into two parts by our first selection of *f1* with threshold *0.5*.
How is it reflected for thee *tree*?

When building the index, we're essentially creating a *binary search tree*. The first selection of *f1* with threshold *0.5* became our root node.
Given each data point, if the *f1* is greater than .5, it will be placed to the right of the node, otherwise to the left of the node.

**iteration 2**

we continue from the tree above. In the second iteration, let's define our rule as: given *f1 > 0.5*, select *f2* with threshold *0.5*. 

![kdtree-2](images/kdtree-2.png)

As was shown in the graph above, now we splited the feature space again based on the new rule, and it is also reflected on our tree: we created a new Node *f2-0.5* into the figure (the none Node is only for visualization purpose, we havn't create this Node).

**iteration N**

If we follow this process, we will end up with:

![kdtree-3](images/kdtree-3.png)

As was shwon in the above figure, the entire feature space has been splitted into 6 bins. Compared with before, we added 3 new Nodes including 2 leaf Nodes:

1. the previous 'none' was replaced by a real node `f2-0.65`, this node split the space of *f2* based on threshold 0.65, and it only happens when *f1<0.5*.
2. when *f2<0.65*, we further split *f1* by a threshold of 0.2.
3. when *f2>0.65*, we further split *f1* by a threshold of 0.3.

To this end, our tree has 3 leaf nodes and each leaf node is able to construct 2 bins (less/larger than threshold), and we have 6 bins in total.
And each data point can be placed into 1 on the bins.

Then we finish the construction of the KD-Tree. It should be noted that constructing a KD-Tree could be non-trival since you need to consider some hyper-parameters, such as how to set the threshold or how many bins should we create (or the stop criteria). In practice there is no golden rules, normally mean or median can be used to set as threshold. Number of bins, could be highly dependent on evaluation of the results and fine-tuning.


### Nearest Neighbour: KD-Tree Searching

Searching time when user uploading an image illustrated as the green x in the image below:

![kdtree-4](images/kdtree-search.png)

We can easily place the search article into the *top-left bin*. Within that bin, it can be seen that we have 1 item can be considered as candidate nearest neighbours. While it's not finished yet.

Given this query data, we also need to compute the mimimal distance between the query image and all-other-bins. More specifically, not the bins itself, but the bounding box (represneted as rectangles inside each bins).

If the mimimal distance between the query vector is:

1. greater than the distance against candidate in the same bin, we ignore this bin by prune the leaf node.
2. less than the distance agains candidate in the same bin, we add this bin (and all items inside the bin) as candidates.


To this end, our search space can be greatly reduced: we do not need to conduct brute-force search, search through several bins is sufficiently enough. In the above case, is 2.

### Approximate Nearest Neighbour: KD-Tree Searching

ANN Search for KD-Tree is extremly easy, the idea is pretty similar as what we're doing for hard-negative mining: adding a margin to the distance.

In the KD-Tree NN search case, given the query and candidate item in the same bin, let's say the distance between them is $d(q, candidate)$.
After then we also compute the minimal distance between the query and the bounding boxes, in the above example, we'll get 5 other distances.
If the distance between the query and the bounding box is less than $d(q, candidate)$, we include all the items inside the bin.

ANN search works exactly the same, while we penalize the $d(q, candidate)$ a little bit by divide $d(q, candidate)/factor$ where $factor \in [1, \infty)$.

In this case, if *factor=1*, it is identical to NN search. If *factor>1*, the distance between query and bounding boxes has to be even smaller than the penalized value in order to be considered as *nearest neighbours*.

In practice, it works much more efficiently than NN search while still returns a list of nearest neighbours with quality. 

I'll conclude KD-Tree here. In the next blog, we'll discuss a bit into the pros and cons of KD-Tree, and dive deep into locally sensitive hashing based NN/ANN search.

Last but not least, thanks for [plotly](https://plotly.com/graphing-libraries/) to help me make beautiful figures!